<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Youtian Lin</title>

    <meta name="author" content="Youtian Lin">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Youtian Lin
                </p>
                <p>I am a first-year Ph.D. student at <a href="https://www.nju.edu.cn/en/">Nanjing University</a>, under the supervision of Prof. <a href="https://yoyo000.github.io/index.html">Yao Yao</a>. My research focuses on 4D/3D reconstruction and generation. Previously, I pursued a Ph.D. at the <a href="https://www.hit.edu.cn">Harbin Institute of Technology</a>. I earned my M.S. from the <a href="https://english.hrbeu.edu.cn">Harbin Engineering University</a> in 2021, where I was advised by Prof. <a href="https://www.researchgate.net/profile/Jian-Guan-24">Jian Guan</a>.
                </p>
                <p style="text-align:center">
                  <a href="mailto:linyoutian.loyot@gmail.com">Email</a> &nbsp;/&nbsp;
                  <!-- <a href="data/JonBarron-CV.pdf">CV</a> &nbsp;/&nbsp; -->
                  <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp;/&nbsp; -->
                  <a href="https://scholar.google.com/citations?hl=zh-CN&user=VhhHLhIAAAAJ">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://twitter.com/linyoutian2">Twitter (X)</a> &nbsp;/&nbsp;
                  <a href="https://github.com/Linyou">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/IMG_0284.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/IMG_0284.JPG" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  My research focuses on the intersection of computer vision and computer graphics, aiming to bridge the gap between 3D and Vision. Currently, I specialize in 4D/3D reconstruction and generation, with an emphasis on real-world applications.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <img src='images/direct3d.png' width="160">
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2405.14832">
          <span class="papertitle">Direct3D: Scalable Image-to-3D Generation via 3D Latent Diffusion Transformer</span>
        </a>
        <br>
        <a href="https://scholar.google.it/citations?user=SN8J78EAAAAJ&hl=zh-CN">Shuang Wu*</a>
        <strong>Youtian Lin*</strong>
        <a href="http://www.feihuzhang.com">Feihu Zhang</a>
        <a href="https://github.com/zeng-yifei">Yifei Zeng</a>
        <a href="https://www.researchgate.net/profile/Jingxi-Xu-2">Jingxi Xu</a>
        <a href="https://scholar.google.com/citations?user=kPxa2w0AAAAJ&hl=zh-CN">Philip Torr</a>
        <a href="https://cite.nju.edu.cn/People/Faculty/20190621/i5054.html">Xun Cao</a>
        <a href="https://yoyo000.github.io/index.html">Yao Yao</a>
        <br>
        <em>Arxiv</em>, 2024 &nbsp
        <!-- <font color="red"><strong>(Oral Presentation)</strong></font> -->
        <br>
        <a href="https://nju-3dv.github.io/projects/Direct3D/">Project Page</a>
        <!-- /
        <a href="https://arxiv.org/abs/2209.14988">arXiv</a>
        /
        <a href="https://github.com/Linyou/Ced-NeRF">gallery</a> -->
        <p></p>
        <p>
          Direct3D introduces a scalable approach for generating high-quality 3D assets from images. It uses D3D-VAE for efficient 3D shape encoding and D3D-DiT for modeling 3D latents. This method setting a new standard for 3D content creation. 
        </p>
      </td>
    </tr>



    <td style="padding:20px;width:25%;vertical-align:middle">
      <div class="one">
        <img src='images/stag4d.png' width="160">
      </div>
    </td>
    <td style="padding:20px;width:75%;vertical-align:middle">
      <a href="https://arxiv.org/abs/2403.14939">
        <span class="papertitle">STAG4D: Spatial-Temporal Anchored Generative 4D Gaussians</span>
      </a>
      <br>
      <a href="https://github.com/zeng-yifei">Yifei Zeng*</a>
      <a href="https://github.com/yanqinJiang">Yanqin Jiang*</a>
      <a href="https://sites.google.com/site/zhusiyucs/home/">Siyu Zhu</a>
      <a href="https://github.com/YuanxunLu">Yuanxun Lu</a>
      <strong>Youtian Lin</strong>
      <a href="http://zhuhao.cc/">Hao Zhu</a>
      <a href="https://people.ucas.ac.cn/~huweiming">Weiming Hu</a>
      <a href="https://cite.nju.edu.cn/People/Faculty/20190621/i5054.html">Xun Cao</a>
      <a href="https://yoyo000.github.io/index.html">Yao Yao</a>
      <br>
      <em>Arxiv</em>, 2024 &nbsp
      <!-- <font color="red"><strong>(Oral Presentation)</strong></font> -->
      <br>
      <a href="https://github.com/zeng-yifei/STAG4D">Github</a> &
      <a href="https://nju-3dv.github.io/projects/STAG4D/">Project Page</a>
      <!-- /
      <a href="https://arxiv.org/abs/2209.14988">arXiv</a>
      /
      <a href="https://github.com/Linyou/Ced-NeRF">gallery</a> -->
      <p></p>
      <p>
        High-fidelity 4D generation from diverse inputs (text, image, and video) with pre-trained diffusion models and dynamic 3D Gaussian splatting. 
      </p>
    </td>
  </tr>

    
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="one">
      <img src='images/gaussian-flow.png' width="160">
    </div>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://arxiv.org/abs/2312.03431">
      <span class="papertitle">Gaussian-Flow: 4D Reconstruction with Dynamic 3D Gaussian Particle</span>
    </a>
    <br>
    <strong>Youtian Lin</strong>,
    <a href="https://scholar.google.co.jp/citations?hl=zh-CN&user=1W9jiEEAAAAJ">Zuozhuo Dai</a>,
    <a href="https://sites.google.com/site/zhusiyucs/home">Siyu Zhu</a>,
    <a href="https://yoyo000.github.io/index.html">Yao Yao</a>
    <br>
    <em>CVPR</em>, 2024 (<font color="red">Highlight: 2.8%</font>) &nbsp
    <!-- <font color="red"><strong>(Oral Presentation)</strong></font> -->
    <br>
    <a href="https://github.com/pointrix-project">Github</a> &
    <a href="https://nju-3dv.github.io/projects/Gaussian-Flow/">Project Page</a>
    (The code will be released with <a href="https://github.com/DSaurus/threestudio-3dgs"><strong>pointrix</strong></a> soon, stay tuned!)
    <!-- /
    <a href="https://arxiv.org/abs/2209.14988">arXiv</a>
    /
    <a href="https://github.com/Linyou/Ced-NeRF">gallery</a> -->
    <p></p>
    <p>
      We propose an innovative point-based method for rapid dynamic scene reconstruction and real-time rendering from both multi-view and monocular videos, leveraging advancements in point-based 3D Gaussian Splatting (3DGS). 
    </p>
  </td>
</tr>

<td style="padding:20px;width:25%;vertical-align:middle">
  <div class="one">
    <img src='images/brdf-3dgs-2.png' width="160">
  </div>
</td>
<td style="padding:20px;width:75%;vertical-align:middle">
  <a href="https://arxiv.org/abs/2311.16043">
    <span class="papertitle">Relightable 3D Gaussian: Real-time Point Cloud Relighting with BRDF Decomposition and Ray Tracing</span>
  </a>
  <br>
  <a href="https://ygaojiany.github.io">Jian Gao</a>,
  <a href="https://sulvxiangxin.github.io">Chun Gu</a>,
  <strong>Youtian Lin</strong>,
  <a href="http://zhuhao.cc/home/">Hao Zhu</a>,
  <a href="https://cite.nju.edu.cn/People/Faculty/20190621/i5054.html">Xun Cao</a>,
  <a href="https://lzrobots.github.io">Li Zhang</a>,
  <a href="https://yoyo000.github.io/index.html">Yao Yao</a>
  <br>
  <em>Arxiv</em>, 2023 &nbsp 
  <!-- <font color="red"><strong>(Oral Presentation)</strong></font> -->
  <br>
  <a href="https://github.com/NJU-3DV/Relightable3DGaussian">Github</a> &
  <a href="https://nju-3dv.github.io/projects/Relightable3DGaussian/">Project Page</a>
  <!-- /
  <a href="https://arxiv.org/abs/2209.14988">arXiv</a>
  /
  <a href="https://github.com/Linyou/Ced-NeRF">gallery</a> -->
  <p></p>
  <p>
  Utilizuing 3D Gaussian points to represent a scene, allowing for material and lighting decomposition, enabling real-time relighting, ray-tracing, and editing of the 3D point cloud with improved BRDF estimation and novel view rendering results. 
  </p>
</td>
</tr>
<td style="padding:20px;width:25%;vertical-align:middle">
  <div class="one">
    <img src='images/unidream-1.png' width="160">
  </div>
</td>
<td style="padding:20px;width:75%;vertical-align:middle">
  <a href="https://yg256li.github.io/UniDream/">
    <span class="papertitle">UniDream: Unifying Diffusion Priors for Relightable Text-to-3D Generation</span>
  </a>
  <br>
  <a href="https://scholar.google.co.jp/citations?hl=zh-CN&user=bkPJckwAAAAJ">Zexiang Liu*</a>,
  <a href="https://scholar.google.com/citations?user=a7AMvgkAAAAJ&hl=zh-CN">Yangguang Li*</a>,
  <strong>Youtian Lin*</strong>
  <a href="https://scholar.google.com/citations?user=JX8kSoEAAAAJ&hl=zh-CN">Xin Yu</a>,
  <a href="https://pengsida.net">Sida Peng</a>,
  <a href="https://yanpei.me">Yan-Pei Cao</a>,
  <a href="https://xjqi.github.io">Xiaojuan Qi</a>,
  <a href="https://www.researchgate.net/profile/Jian-Guan-24">Xiaoshui Huang</a>,
  <a href="https://scholar.google.com/citations?user=Dqjnn0gAAAAJ&hl=zh-CN">Ding Liang</a>,
  <a href="https://wlouyang.github.io">Wanli Ouyang</a>
  <br>
  <em>Arxiv</em>, 2023 &nbsp 
  <!-- <font color="red"><strong>(Oral Presentation)</strong></font> -->
  <br>
  <a href="https://github.com/YG256Li/UniDream">Github</a> &
  <a href="https://yg256li.github.io/UniDream/">Project Page</a>
  <!-- /
  <a href="https://arxiv.org/abs/2209.14988">arXiv</a>
  /
  <a href="https://github.com/Linyou/Ced-NeRF">gallery</a> -->
  <p></p>
  <p>
    Use a dual-phase training process for albedo-normal aligned multi-view diffusion and reconstruction models, a progressive generation procedure for geometry and albedo-textures using Score Distillation Sample (SDS), and an innovative SDS application for finalizing Physically Based Rendering (PBR) generation with fixed albedo. 
  </p>
</td>
</tr>
<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="one">
      <img src='images/ced-nerf2.png' width="160">
    </div>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://github.com/Linyou/Ced-NeRF">
      <span class="papertitle">Ced-NeRF: A Compact and Efficient Method for Dynamic Neural Radiance Fields</span>
    </a>
    <br>
    <strong>Youtian Lin</strong>
    <br>
    <em>AAAI</em>, 2024 &nbsp 
    <!-- <font color="red"><strong>(Oral Presentation)</strong></font> -->
    <br>
    <a href="https://github.com/Linyou/Ced-NeRF">Github</a>
    <!-- /
    <a href="https://arxiv.org/abs/2209.14988">arXiv</a>
    /
    <a href="https://github.com/Linyou/Ced-NeRF">gallery</a> -->
    <p></p>
    <p>
    We extend the Instant-NGP framework to support dynamic scenes, and show that it can be used to train a dynamic NeRF model that is both more compact and more efficient than prior work. 
    </p>
  </td>
</tr>
<td style="padding:20px;width:25%;vertical-align:middle">
  <div class="one">
    <img src='images/earl.png' width="160">
  </div>
</td>
<td style="padding:20px;width:75%;vertical-align:middle">
  <a href="https://ieeexplore.ieee.org/document/10238752">
    <span class="papertitle">EARL: An Elliptical Distribution aided Adaptive Rotation Label Assignment for Oriented Object Detection in Remote Sensing Images</span>
  </a>
  <br>
  <a href="https://www.researchgate.net/profile/Jian-Guan-24">Jian Guan</a>,
  <a href="https://justlovesmile.github.io/">Mingjie Xie</a>,
  <strong>Youtian Lin</strong>,
  <a href="https://scholar.google.com/citations?user=a7AMvgkAAAAJ&hl=zh-CN">Guangjun He</a>,
  <a href="https://scholar.google.co.jp/citations?hl=zh-CN&user=O51mMKgAAAAJ">Pengming Feng</a>
  <br>
  <em>IEEE TGRS</em>, 2023 &nbsp 
  <!-- <font color="red"><strong>(Oral Presentation)</strong></font> -->
  <br>
  <a href="https://github.com/Justlovesmile/EARL">Github</a>
  <p></p>
  <p>
  Incorporating adaptive scale sampling, dynamic elliptical distribution aided sampling, and spatial distance weighting to enhance the selection of high-quality positive samples.
  </p>
</td>
</tr>
<td style="padding:20px;width:25%;vertical-align:middle">
  <div class="one">
    <img src='images/toso.png' width="160">
  </div>
</td>
<td style="padding:20px;width:75%;vertical-align:middle">
  <a href="https://ieeexplore.ieee.org/document/9053562">
    <span class="papertitle">TOSO: Student's-T Distribution Aided One-Stage Orientation Target Detection in Remote Sensing Images</span>
  </a>
  <br>
  <a href="https://scholar.google.co.jp/citations?hl=zh-CN&user=O51mMKgAAAAJ">Pengming Feng*</a>,
  <strong>Youtian Lin*</strong>,
  <a href="https://www.researchgate.net/profile/Jian-Guan-24">Jian Guan</a>,
  <a href="https://scholar.google.com/citations?user=a7AMvgkAAAAJ&hl=zh-CN"> Guangjun He</a>,
  <a href="https://linyou.github.io/youtian-page/">Huifeng Shi</a>,
  <a href="https://scholar.google.co.jp/citations?hl=zh-CN&user=UeOcj28AAAAJ">Jonathon Chambers</a>
  <br>
  <em>ICASSP</em>, 2020 &nbsp 
  <!-- <font color="red"><strong>(Oral Presentation)</strong></font> -->
  <br>
  <!-- <a href="https://github.com/Linyou/Ced-NeRF">Project Page</a> -->
  <!-- /
  <a href="https://arxiv.org/abs/2209.14988">arXiv</a>
  /
  <a href="https://github.com/Linyou/Ced-NeRF">gallery</a> -->
  <p></p>
  <p>
  Utilizing a one-stage keypoint based network architecture and introducing a novel geometric transformation method to achieve orientation angle regression, along with incorporating Student's-t distribution to enhance performance
  </p>
</td>
</tr>
<td style="padding:20px;width:25%;vertical-align:middle">
  <div class="one">
    <img src='images/ienet.png' width="160">
  </div>
</td>
<td style="padding:20px;width:75%;vertical-align:middle">
  <a href="https://arxiv.org/abs/1912.00969">
    <span class="papertitle">IENet: Interacting Embranchment One Stage Anchor Free Detector for Orientation Aerial Object Detection</span>
  </a>
  <br>
  <strong>Youtian Lin</strong>,
  <a href="https://scholar.google.co.jp/citations?hl=zh-CN&user=O51mMKgAAAAJ">Pengming Feng</a>,
  <a href="https://www.researchgate.net/profile/Jian-Guan-24">Jian Guan</a>,
  <a href="https://scholar.google.co.jp/citations?hl=zh-CN&user=JQFnV5IAAAAJ">Wenwu Wang</a>,
  <a href="https://scholar.google.co.jp/citations?hl=zh-CN&user=UeOcj28AAAAJ">Jonathon Chambers</a>
  <br>
  <em>Arxiv</em>, 2019 &nbsp 
  <!-- <font color="red"><strong>(Oral Presentation)</strong></font> -->
  <br>
  <p></p>
  <p>
  We addressing the challenges of computational complexity in two-stage detectors by employing a per-pixel prediction approach with a geometric transformation, a branch interactive module, and an enhanced intersection over union (IoU) loss. 
  </p>
</td>
</tr>

  </tbody></table>
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
    <tr>
      <td>
        <h2>Project</h2>
      </td>
    </tr>
  </tbody></table>
  <table width="100%" align="center" border="0" cellpadding="20"><tbody>
  <tr>
    <td style="padding:20px;width:25%;vertical-align:middle">
      <img src="images/github-mark.png" alt="piano" width="120"></td>
    <td width="75%" valign="center">
    <p>
      <a href="https://github.com/DSaurus/threestudio-3dgs">
        <papertitle>pointrix-project</papertitle> 
        </a><br>
        Pointrix: a differentiable point-based rendering library. 
      <br><br>
      <a href="https://github.com/DSaurus/threestudio-3dgs">
        <papertitle>threestudio-3dgs</papertitle> 
        </a><br>
        The Gaussian Splatting extension for threestudio. 
      <br><br>
      <a href="https://github.com/taichi-dev/taichi-nerfs">
      <papertitle>taichi-nerfs</papertitle>
      </a>
      <br>
      A PyTorch + Taichi implementation of instant-ngp NeRF training pipeline. For more details about modeling, please checkout <a href="https://docs.taichi-lang.org/blog/taichi-instant-ngp">this blog</a>.
      <br><br>
      <a href="https://github.com/Linyou/taichi-ngp-renderer">
      <papertitle>taichi-ngp-renderer</papertitle> 
      </a><br>
      A Instant-NGP renderer implemented using Taichi, written entirely in Python. No CUDA! 
      <br><br>
      <a href="https://github.com/nerfstudio-project/nerfacc">
        <papertitle>nerfacc</papertitle> 
        </a><br>
        A PyTorch Nerf acceleration toolbox for both training and inference. (<strong>As a contributor, I have implemented a fast ngp rendering in CUDA</strong>) 
      <br><br>
    </p>
    </td>
  </tr>
  </tbody></table>
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr>
      <td style="padding:0px">
        <br>
        <p style="text-align:right;font-size:small;">
          Source code borrow from <a href="https://github.com/jonbarron/jonbarron_website">jonbarron</a>.
        </p>
      </td>
    </tr>
  </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
